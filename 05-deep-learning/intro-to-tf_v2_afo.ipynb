{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Data-X: Introduction to TensorFlow\n",
    "### Computation Graphs, Simple One Layer ANN, Vanilla DNN\n",
    "\n",
    "**Author:** Alexander Fred Ojala\n",
    "\n",
    "**Sources:** Sebastian Raschka, Aurélien Géron, etc.\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General notebook setup\n",
    "Make notebook compatible with Python 2 and 3, plus import standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pyton 2 and 3 support\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to TensorFlow\n",
    "### Core components:\n",
    "\n",
    "#### 1. Tensor\n",
    "A Tensor in TensorFlow is an N-dimensional array (just like Numpys array object). Tensors are multilinear maps from vector spaces to real numbers. Scalars, vectors and matrices are all tensors. Think of tensors as a multi-dimensional array. The Tensor represents units of data in TensorFlow.\n",
    "\n",
    "Numpy arrays are automatically converted into TensorFlow tensors. They cannot be inserted into tensor functions, can’t automatically compute derivatives and computations cannot automatically be sent to a GPU. (However, it is usually simple to do quick computations in NumPy).\n",
    "\n",
    "#### 2. Operations / Ops\n",
    "TensorFlow operations or ops are units / edges / nodes of computation (e.g. matrix multiplication, addition, etc.)\n",
    "\n",
    "#### 3. Computation Graph\n",
    "The computational graph is the core component of TensorFlow it represents the dataflow and the order of computations. First we build a graph (nothing is calculated), then this computation graph is  sent to another instance / runtime environment (e.g. on a CPU or GPU) for execution. The results are sent back to us. This makes TensorFlow computations highly distributable and it also allows us to automatically evaluate all gradients in the computation nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TensorFlow benefits:**\n",
    "- Highly efficient\n",
    "- Cross-platform (works on IOS, Android, Unix, Windows, in the cloud etc etc)\n",
    "- Calculates gradients automatically (this is truly useful for Neural Networks, where the analytical solution of gradients would be VERY tedious to derive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Canonical way of importing TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# If this doesn't work TensorFlow is not installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tf version, oftentimes tensorflow is not backwards compatible\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: When using Jupyter notebook make sure to call tf.reset_default_graph() at the beginning to clear the symbolic graph before defining new nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard setup\n",
    "Tip2: Setup TensorBoard to monitor graph etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "t = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") \n",
    "log_dir = \"tf_logs\"\n",
    "logd = \"/tmp/{}/r{}/\".format(log_dir, t)\n",
    "\n",
    "# Then every time you have specified a graph run:\n",
    "# file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "logdir = os.path.join(os.sep,home,logd)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>Please confirm that this `Tensorboard` setup works on Windows!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard Graph visualizer in notebook\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow operations are not executed directly, but rather they are placed in a logical order in a computation graph. In order to evaluate them, we have to run them in a session (shown later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 tf.constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are initialized directly and we cannot change the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors can also have names (in the computation graph)\n",
    "named_tensor = tf.constant(7.2, name='my_named_tuple')\n",
    "named_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 tf.placeholder\n",
    "\n",
    "Placeholders can be fed an arbitrary or specified amount of data at anytime in the execution flow. They are not initialized directly, but need a data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we have to specify a data type\n",
    "# just as for the numpy array\n",
    "# shape is optional\n",
    "# here 3 columns, arbitrary number of rows\n",
    "c = tf.placeholder(dtype=tf.float64,shape=[None,3])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow datatypes: https://www.tensorflow.org/api_docs/python/tf/DType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 tf.Variable\n",
    "\n",
    "A Variable is NOT initiliazed directly, but have to be initialzed in the session. The variables can also be updated and reassigned new values. Variables are usually all the weights and biases that are optimized during training, they also indicate the degrees of freedom of the model (what model parameters that can change, thus making the model flexible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf.Variable(np.random.randn(3).reshape(3,1)) #reshape\n",
    "# automatically assings data type\n",
    "d #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>Tensorflow is really similar to NumPy, and you can think of the tensors as an ndimensional array.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tf_to_np](imgs/tf_to_np.png)\n",
    "Source: CS227d, NLP, Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operations / Ops\n",
    "Operations can be carried out directly or assigned to variables. If they are assigned to variables, the variable will be an operation node in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op1 = tf.add(a,b) # nothing happens\n",
    "op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a+b # same as tf.add, here the operation is not saved in the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify ops on arrays and matrices\n",
    "# however the shapes have to be adequate\n",
    "\n",
    "# Here we do an operation on a placeholder and a Variable\n",
    "# Same as (functional maps)\n",
    "print('c.shape=',c.shape, 'd.shape=',d.shape)\n",
    "op2 = tf.matmul(c,d) # note we have to \"feed data\" to d when we evaluate this\n",
    "op2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow graphs, calculations & executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph based computations are powerful, because the computations can be split up and run in parallell really easy. For example:\n",
    "\n",
    "* a = b + c\n",
    "* d = b + 2\n",
    "\n",
    "These two computations can be sent to different CPUs / GPUs. These gains are remarkable for complex NNs such as CNNs and RNNs.\n",
    "\n",
    "____\n",
    "\n",
    "Graph based computations also allow tensorflow to implement solutions for automatic gradient calculations at every node (as we're working with symbolic computational graphs). TensorFlow builds a graph first in order to do formal full gradient calculation on all nodes (before running / executing the operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate nodes and operations with `tf.Session()` objects\n",
    "\n",
    "Use the `.run()` method on a Session and evaluate tensors and execute operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session() # start a session\n",
    "\n",
    "# A session object encapsulates the environment where we evaluate tensors and execute ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tf.Variable(2, name = 'f')\n",
    "sess.run(f.initializer) # !!! error if we don't\n",
    "sess.run(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is tedious if we have many variables\n",
    "\n",
    "# Instead initialize all variables globally\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op1)\n",
    "sess.run(op1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(a+b) # a = 2, b = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must feed the placeholder node c data\n",
    "tmp_data = np.array([[3,2,1],[1,2,3]])\n",
    "tmp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(c, \\\n",
    "         feed_dict={c:tmp_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex cmoputational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = a+b\n",
    "u = v+2\n",
    "w = v*u\n",
    "z = w*3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we run z then tensorflow will recognize that it has to evaluate\n",
    "# w, u and v in order to get the result ofz\n",
    "sess.run(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We defined operations v, u and w which need to be calculated before we can figure out what z is.  However, we don’t have to explicitly run those operations, as TensorFlow knows what other operations and variables the operation a depends on, and therefore runs the necessary operations on its own.  It does this through its data flow graph which shows it all the required dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `with` sessions\n",
    "Oftentimes you will also see sessions defined within a `with` environment. This is great when you want to send computations to different run time environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess2:\n",
    "    print(sess2.run(a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A graph is launched with a Session.\n",
    "# To see what graph we're currently working in:\n",
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define different Graph environments\n",
    "1. First create the graph and the nodes in the graph for the specific computations\n",
    "2. Then initialize a session for the specific graph\n",
    "\n",
    "Note: If no graph is defined, TensorFlow will automatically assign the computation nodes to the default graph `tf.get_default_graph()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define new graph\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default() as graph:\n",
    "    a = tf.placeholder(tf.float32, name='a') # placeholders without shape\n",
    "    b = tf.placeholder(tf.float32, name='b')\n",
    "    multiply_node = tf.multiply(a,b, name = 'a_b')\n",
    "    mult_add_5 = multiply_node + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess2 = tf.Session(graph=graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess2.run(mult_add_5, feed_dict={a:2.3, b:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with block Session on graph 2\n",
    "with tf.Session(graph=graph2) as s:\n",
    "    res = s.run(multiply_node, feed_dict = {a: 3, b: 4.5})\n",
    "    print('3*4.5 =',res)\n",
    "    print()\n",
    "    # we can also get vectorized results\n",
    "    print(s.run(mult_add_5, {a: np.ones(4), b: range(2,6)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph()) # to get the graph we automatically created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namescopes to group computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph3 = tf.Graph()\n",
    "\n",
    "with graph3.as_default():\n",
    "\n",
    "    with tf.name_scope('hidden') as scope:\n",
    "        x = tf.constant(2, name='x')\n",
    "        y = tf.constant(3, name='y')\n",
    "        z = tf.multiply(x,y, name='z')\n",
    "    with tf.name_scope('second') as scope:\n",
    "        w1 = tf.add(z, 3, name='w1')\n",
    "        w2 = z - 4\n",
    "    sess3 = tf.Session(graph=graph3)\n",
    "    print(sess3.run(w1))\n",
    "    print(sess3.run(w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write graph as TensorBoard file\n",
    "file_writer = tf.summary.FileWriter(logdir, graph3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tensorboard\n",
    "# NOTE: You could do this in the terminal (recommended)\n",
    "# You have to interrupt kernel to continue executions\n",
    "\n",
    "!tensorboard --logdir=$logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in TensorFlow\n",
    "\n",
    "Based on Google docs example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables can be chosen as model parameters\n",
    "# TensorFlow can help us optimize them\n",
    "# by automatically calculating the gradient\n",
    "\n",
    "# Let's try to fit a simple lienar regression model\n",
    "# y = theta_0 + theta_1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data we want to fit to:\n",
    "\n",
    "x_real = np.linspace(0,10)\n",
    "y_real = 5.0 + 2.0*x_real + np.random.normal(0,2,len(x_real),)\n",
    "\n",
    "plt.scatter(x_real, y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the variables at random values\n",
    "theta0 = tf.Variable([.1], dtype=tf.float32, name='bias')\n",
    "theta1 = tf.Variable([.3], dtype=tf.float32, name='weight')\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "\n",
    "y = tf.placeholder(tf.float32, name='y') # placeholder for ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model as a variable\n",
    "linear_model = theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are not initialized when we call tf.Variable\n",
    "# To initialize them we must tell TensorFlow to do it\n",
    "theta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since x is a placeholder, we can evaluate linear_model for\n",
    "# several values of x\n",
    "sess.run(linear_model, {x: range(5)})\n",
    "\n",
    "# only after we have initialized the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function, so that we can evaluate\n",
    "# how good our model is and train it\n",
    "\n",
    "SE = tf.square(linear_model - y) # square elements\n",
    "MSE = tf.reduce_sum(SE) # sum of elements\n",
    "loss = MSE\n",
    "dictfeed = {x: x_real, y: y_real}\n",
    "\n",
    "sess.run(loss, dictfeed)\n",
    "\n",
    "# the loss is really high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss was really big for those parameters.\n",
    "# We can also reassign the values of the weights\n",
    "theta0_update = tf.assign(theta0, [2.])\n",
    "theta1_update = tf.assign(theta1, [2.])\n",
    "sess.run([theta0_update, theta1_update]) # run the assign operation\n",
    "# to update the variables\n",
    "\n",
    "sess.run(loss, dictfeed)\n",
    "# reduced loss # closer to the real values, still really high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function. The simplest optimizer is gradient descent. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. \n",
    "\n",
    "In general, computing symbolic derivatives manually is tedious and error-prone. Consequently, TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients. For simplicity, optimizers typically do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model with GradientDescent iteratively\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = .0001)\n",
    "\n",
    "# train step\n",
    "train = optimizer.minimize(loss) # training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init) # init operation re-initilize weight values\n",
    "sess.run([theta0,theta1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    sess.run(train, {x: x_real, y: y_real})\n",
    "    # here is where the magic happens, the gradients are calculated \n",
    "    # and they are updated in every iteration\n",
    "    # in accordance with a step in the negative \n",
    "    # gradient's direction according to the step size we have chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,W = sess.run([theta0, theta1]) # correct values are 5 and 2\n",
    "print(b,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_real,y_real)\n",
    "plt.plot(x_real, b+W*x_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST: Intro to NN in TensorFlow\n",
    "\n",
    "Example taken from Google Docs\n",
    "\n",
    "We are now going to recognize hand-written digits.\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST.png](https://www.tensorflow.org/images/MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the most classic NN dataset\n",
    "The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation). This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!\n",
    "\n",
    "Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. We'll call the images \"x\" and the labels \"y\". Both the training set and test set contain images and their corresponding labels; for example the training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "\n",
    "Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "![https://www.tensorflow.org/images/MNIST-Matrix.png](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "We can flatten this array into a vector of 28x28 = 784 numbers. It doesn't matter how we flatten the array, as long as we're consistent between images. From this perspective, the MNIST images are just a bunch of points in a 784-dimensional vector space, with a very rich structure (warning: computationally intensive visualizations).\n",
    "\n",
    "Flattening the data throws away information about the 2D structure of the image. Isn't that bad? Well, the best computer vision methods do exploit this structure, and we will in later tutorials. But the simple method we will be using here, a softmax regression (defined below), won't.\n",
    "\n",
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [55000, 784]. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.\n",
    "\n",
    "![https://www.tensorflow.org/images/mnist-train-xs.png](https://www.tensorflow.org/images/mnist-train-xs.png)\n",
    "\n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.\n",
    "\n",
    "For the purposes of this tutorial, we're going to want our labels as \"one-hot vectors\". A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the $n$th digit will be represented as a vector which is 1 in the $n$th dimension. For example, 3 would be $[0,0,0,1,0,0,0,0,0,0]$. Consequently, mnist.train.labels is a [55000, 10] array of floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax regression\n",
    "\n",
    "We know that every image in MNIST is of a handwritten digit between zero and nine. So there are only ten possible things that a given image can be. We want to be able to look at an image and give the probabilities for it being each digit. For example, our model might look at a picture of a nine and be 80% sure it's a nine, but give a 5% chance to it being an eight (because of the top loop) and a bit of probability to all the others because it isn't 100% sure.\n",
    "\n",
    "This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities.\n",
    "\n",
    "**Softmax regression equation**\n",
    "\n",
    "$$\\sigma (\\mathbf {z} )_{j}={\\frac {e^{z_{j}}}{\\sum _{k=1}^{K}e^{z_{k}}}}\\ for\\ j = 1, …, K$$\n",
    "\n",
    "where $z$ represents the values from the output layer and $K$ represents the number of output classes.\n",
    "\n",
    "To tally up the evidence that a given image is in a particular class, we do a weighted sum of the pixel intensities. The weight is negative if that pixel having a high intensity is evidence against the image being in that class, and positive if it is evidence in favor.\n",
    "\n",
    "The following diagram shows the weights one model learned for each of these classes. Red represents negative weights, while blue represents positive weights.\n",
    "\n",
    "![https://www.tensorflow.org/images/softmax-weights.png](https://www.tensorflow.org/images/softmax-weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Vanilla) ANN Network structure\n",
    "* Any Neural Network with one hidden layer can be a Universal Function Approximator. Source: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "* The number of input nodes are equal to the number of features\n",
    "* The number of output nodes are equal to the number of classes (for classification tasks)\n",
    "* A bias term is added to every layer that only feeds in a 1, that adds an extra degree of freedom for every functional input value to the next function\n",
    "\n",
    "# How deep should we go?\n",
    "* We can overfit Neural Nets, one way to combat that is by using dropout and regularization\n",
    "* Predictions will usually be better when we increase depth of network and widen it (increase the number of neurons in every layer)\n",
    "\n",
    "# Activation Functions\n",
    "* Classically the sigmoid function was used in the hidden layers (simplest function between 0 - 1). Logit function.\n",
    "* Nowadays it is more common to use the ReLU (Rectified Linear Unit). Much quicker! For deep networks sigmoid might not want to converge at all. Much better to handle exploding and vanishing gradients (Leaky Relu). Can also combat that with *Batch Normalization*.\n",
    "* For the input layer we send in the (standardized) values.\n",
    "* For the output layer we often use a softmax function (multi-class classification) or a sigmoid function (binary classification). Softmax only works if the classes are mutually exclusive, i.e. we only try to label one pattern in every training example.\n",
    "\n",
    "\n",
    "# Training algorithm steps\n",
    "* Train a model to make a prediction\n",
    "* Compute distance between predictions and true values\n",
    "* Modify weights and biases to lower error\n",
    "\n",
    "\n",
    "# Overfitting\n",
    "* Mostly because our network has too many degrees of freedom (neurons in the network)\n",
    "* Can use L1 and L2 regularization on the cost function\n",
    "* Drop out (used to mitigate the effects of too many degrees of freedom)\n",
    "\n",
    "# ANNs are not great at classifying images\n",
    "* We don't make use of the image shapes and curves. Shape info is lost when we flatten arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN One Layer Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will accomplish in this section:\n",
    "\n",
    "- Create a softmax regression function that is a model for recognizing MNIST digits, based on looking at every pixel in the image\n",
    "- Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples (and run our first Tensorflow session to do so)\n",
    "- Check the model's accuracy with our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in input data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# contains info\n",
    "import tensorflow.examples.tutorials.mnist.mnist as mnist_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mnist` is a lightweight class which stores the training, validation, and testing sets as NumPy arrays. It also provides a function for iterating through data minibatches, which we will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls MNIST_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_info.IMAGE_PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnist.train.images[2,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUCTION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input\n",
    "x = tf.placeholder(tf.float32,shape = [None,784]) \n",
    "# None, because we don't specify how many examples we'll look at\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10])) # number of weights\n",
    "b = tf.Variable(tf.zeros([10])) # number of bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = tf.nn.softmax(tf.matmul(x, W) + b) \n",
    "# define what we'll take the softmax activation on\n",
    "\n",
    "# Notice order on x and W (dimensions must match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correct answers\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "# Cross entropy\n",
    "\n",
    "ce = tf.reduce_mean(-tf.reduce_sum( y* tf.log(y_hat),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce # sum over the columns to get cost for every training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor accuracy\n",
    "def acc():\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_hat,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # get batches of training data\n",
    "    # we don't show everything to the network at once\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "    if i%100==0:\n",
    "        acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph()) #not that great, we should probably use scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep MINST\n",
    "\n",
    "Example from the book `Hands-On Machine Learning with Scikit-Learn and TensorFlow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improve the one layer model above, by adding extra layers with ReLU activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.learn (high level API)\n",
    "\n",
    "Predefined models, for convenience. This is for fast model building when you have a standard problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input_data (not as one_hot)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# new folder\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# Assign them to values\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUCTION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "\n",
    "# dense neural network classifier\n",
    "# two layers 300 and 100\n",
    "# 10 clases\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                         feature_columns=feature_cols)\n",
    "\n",
    "# if TensorFlow >= 1.1, make compatible with sklearn\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model, 4000 iterations\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracies\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "print()\n",
    "print('Accuracy',accuracy_score(y_test, y_pred['classes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Let's build a DNN with 2 hidden layers from scratch\n",
    "\n",
    "This is great for understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTRUCTION PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and input size\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for data (inputs and targets)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define neuron layers (ReLU in hidden layers)\n",
    "# We'll take care of Softmax for output with loss function\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    # X input to neuron\n",
    "    # number of neurons for the layer\n",
    "    # name of layer\n",
    "    # pass in eventual activation function\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        \n",
    "        # initialize weights to prevent vanishing / exploding gradients\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        \n",
    "        # Initialize weights for the layer\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        # biases\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        \n",
    "        # Output from every neuron\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
    "                           activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                           activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss function (that also optimizes Softmax for output):\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # logits are from the last output of the dnn\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training step with Gradient Descent\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation to see accuracy\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                            y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\") # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") # or better, use save_path\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
