{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Image Classification with CNNs\n",
    "\n",
    "#### Author: Alexander Fred Ojala\n",
    "\n",
    "**Sources:** \n",
    "* **Training + explanations**: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "**Copright:** Feel free to do whatever you want with this code.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1-4: Build your own Cats vs Dogs binary classifier\n",
    "\n",
    "In this notebook we will implement a CNN that is a binary Cats vs Dogs classifier. We will use Transfer learning and remove the top layer of a pretrained network (VGG16 on Imagenet), extracting the features of the training and validation images and then just training the fully connected top layer of that network. This way we will be able to make accurate predictions even though we just have a small data set.\n",
    "\n",
    "The data conists of 2000 training images (1000 cats and 1000 dogs), 800 validation images (400 cats and 400 dogs), and 100 test images (cats and dogs mixed). Find the data here (or in the Github folder): https://www.dropbox.com/s/a8zo6udq83xsx05/data-x_cnn_data.tar.gz?dl=1\n",
    "\n",
    "The pretrained VGG16 weighs that you should load into your model (in order to extract the bottleneck features) can be downloaded here. I have also included the extracted features so that you can run the code instantly: https://www.dropbox.com/s/fvx7hv5vr8j3sc6/vgg16_weights_features.tar.gz?dl=1\n",
    "\n",
    "\n",
    "The reason why we are using a pretrained network, extracting bottleneck features and training only the top layers is that this is a great way to obtain a high prediction accuracy without having a huge data set and without having to run the training for a long time. It would take up to several days to run this analysis and training your own CNN on a personal computer (in order to obtain the same level of accuracy).\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "## [Part 1: Install Keras Tensorflow + all dependencies](#Part-1:-Install-Keras-+-Tensorflow)\n",
    "\n",
    "## [Part 2: Extract bottleneck features from the data set](#Part-2:-Extract-bottleneck-features-from-the-data-set)\n",
    "\n",
    "## [Part 3: Train the top layer of your CNN](#Part-3:-Train-the-top-layer-of-your-CNN)\n",
    "\n",
    "## [Part 4: Make predicitons on the mixed test images](#Part-4:-Validate-accuracy-and-make-predictions-on-unlabeled-data)\n",
    "\n",
    "# Additional Material\n",
    "\n",
    "## [TRAIN NETWORK TO CLASSIFY 50 IMAGE CLASSES w Theano](#Part-5----right-now-built-for-Theano)\n",
    "\n",
    "## [Train Neural Network from Scratch + Image Augmentation](#Appendix:-Training-a-small-convnet-from-scratch:-80%-accuracy-in-40-lines-of-code)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec1'></div>\n",
    "\n",
    "# Part 1: Install Keras + Tensorflow\n",
    "\n",
    "- **Install TensorFlow:** Run the command `conda install tensorflow`\n",
    "- **Install Keras:** Run the command `conda install keras`\n",
    "\n",
    "See https://keras.io/#installation and https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important set correct backend and image_dim_ordering\n",
    "**Set tensorflow backend and image_dim_ordering tf**\n",
    "\n",
    "set it in the **keras.json** file\n",
    "\n",
    "On mac it is loacted: ``~/.keras/keras.json``\n",
    "\n",
    "#### For Windows: \n",
    "Start up your python-binary and do the following\n",
    "\n",
    "        import os\n",
    "        print(os.path.expanduser('~'))\n",
    "        # >>> C:\\\\Users\\\\Sascha'  # will look different for different OS\n",
    "\n",
    "- This should be the base-directory\n",
    "- Keras will build a folder .keras there where keras.json resides (if it was already created). If it's not there, create it there\n",
    "- Example: C:\\\\Users\\\\Sascha\\\\.keras\\\\keras.json'\n",
    "\n",
    "### Content of my keras.json\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"floatx\": \"float32\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"backend\": \"tensorflow\",\n",
    "    \"image_data_format\": \"channels_last\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Read more at:** https://keras.io/backend/#switching-from-one-backend-to-another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf') # note that we need to have tensorflow dimension ordering still because of the weigths.\n",
    "print('The backend is:',K.backend())\n",
    "import tensorflow as tf\n",
    "print(K.image_dim_ordering()) # should say tf\n",
    "print(tf.__version__) # tested for 1.5.0\n",
    "\n",
    "import keras\n",
    "print(keras.__version__) # tested for 2.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import h5py # to handle weights\n",
    "import os, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation, ZeroPadding2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in and process data\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at files, note all cat images and dog images are unique\n",
    "from __future__ import absolute_import, division, print_function # make it compatible w Python 2\n",
    "import os\n",
    "for path, dirs, files in os.walk('./data'):\n",
    "    print('FOLDER',path)\n",
    "    for f in files[:3]:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of cat training images:', len(next(os.walk('./data/train/cats'))[2]))\n",
    "print('Number of dog training images:', len(next(os.walk('./data/train/dogs'))[2]))\n",
    "print('Number of cat validation images:', len(next(os.walk('./data/validation/cats'))[2]))\n",
    "print('Number of dog validation images:', len(next(os.walk('./data/validation/dogs'))[2]))\n",
    "print('Number of uncategorized test images:', len(next(os.walk('./data/test/catvdog'))[2]))\n",
    "\n",
    "# There should be 1000 train cat images, 1000 train dogs, 400 validation cats, 400 validation dogs, 100 uncategorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "TRAIN_DIR = './data/train/'\n",
    "VAL_DIR = './data/validation/'\n",
    "TEST_DIR = './data/test/' #mixed cats and dogs\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "n_train_samples = 2000\n",
    "n_validation_samples = 800\n",
    "n_epoch = 20\n",
    "n_test_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec2'></a>\n",
    "\n",
    "# Part 2: Extract bottleneck features from the data set\n",
    "\n",
    "In this part you will use the pre-trained VGG network structure (loading in the pretrained VGG16 ImageNET weights). Then you will run your data set through that CNN once to extract the image features.\n",
    "\n",
    "A good explanation on how this works (rewritten from source: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "\n",
    "#### Using the bottleneck features of a pre-trained network: 90% accuracy in 1 min (GPU) / 10 mins (CPU)\n",
    "\n",
    "We are leveraging the predictive power of a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset. Because the ImageNet dataset contains several \"cat\" classes (persian cat, siamese cat...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs. cats classification problem extremely well. The method presented here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet.\n",
    "\n",
    "Here's what the VGG16 architecture looks like:\n",
    "\n",
    "![Image of Yaktocat](https://blog.keras.io/img/imgclf/vgg16_original.png)\n",
    "\n",
    "\n",
    "### Horizontal visualization\n",
    "\n",
    "![https://datatoanalytics.files.wordpress.com/2017/04/vgg161.png](https://datatoanalytics.files.wordpress.com/2017/04/vgg161.png)\n",
    "\n",
    "\n",
    "**Strategy to extract bottleneck features:** We will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training, validation and test data once, recording the output (the \"bottleneck features\" from the VGG16 model: the last activation maps before the fully-connected layers) in three numpy arrays. The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. Note, therefore we will not use data augmentation.\n",
    "\n",
    "Store the bottleneck features as `.npy` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is how the VGG16 net looks in code\n",
    "You can see the full implementation here of the network we're building and loading in: https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py\n",
    "\n",
    "\n",
    "```python\n",
    "    # VGG 16 model architecture\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGEnet Benchmarks (the structure we're using today, 2nd place in 2014)\n",
    "\n",
    "![https://qph.ec.quoracdn.net/main-qimg-fbd17e02f01e60b38ff8ee864c647303](https://qph.ec.quoracdn.net/main-qimg-fbd17e02f01e60b38ff8ee864c647303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving bottleneck features\n",
    "# This can take ~10mins to run\n",
    "\n",
    "#  Run model once to record the bottleneck features using image data generators:\n",
    "\n",
    "def save_bottleneck_features():\n",
    "\n",
    "    from keras import applications\n",
    "    model = applications.vgg16.VGG16(include_top=False, weights='imagenet', \\\n",
    "                                     input_tensor=None, input_shape=(img_width, img_height,3))\n",
    "    \n",
    "    print('TensorFlow VGG16 model architecture loaded')\n",
    "    # include_top = False, because we drop last layer, then we also only need to\n",
    "    # download weight file that is small\n",
    "    # input_shape with channels last for tensorflow\n",
    "    \n",
    "    # Our original images consist in RGB coefficients in the 0-255, \n",
    "    # but such values would be too high for our models to process (given typical learning rate), \n",
    "    # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    def generate_features(DIR,n_samples,name_str):\n",
    "        '''This is a generator that will read pictures found in\n",
    "        subfolers of 'data/*', and indefinitely generate\n",
    "        batches of rescaled images used to predict\n",
    "        the bottleneck features of the images once\n",
    "        using model.predict_generator(**args**)'''\n",
    "\n",
    "        print('Generate '+name_str+' image features')\n",
    "    \n",
    "        generator = datagen.flow_from_directory(\n",
    "            DIR,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=1,\n",
    "            class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "            shuffle=False) # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "        \n",
    "        \n",
    "        features = model.predict_generator(generator, n_samples,verbose=True)\n",
    "        # the predict_generator method returns the output of a model, given\n",
    "        # a generator that yields batches of numpy data\n",
    "        \n",
    "        np.save('features_'+name_str+'.npy', features) # save bottleneck features to file\n",
    "    \n",
    "    generate_features(TEST_DIR, n_test_samples, 'test')\n",
    "    #generate_features(TRAIN_DIR, n_train_samples, 'train')\n",
    "    #generate_features(VAL_DIR, n_validation_samples, 'validation')\n",
    "    \n",
    "    print('\\nDone! Bottleneck features have been saved')\n",
    "\n",
    "\n",
    "print('This has been done before the lecture! Takes 5+ mins to run.')\n",
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra\n",
    "# Obtain class labels and binary classification for validation data\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_gen = datagen.flow_from_directory(VAL_DIR,target_size=(img_width, img_height),\n",
    "                                        batch_size=32,class_mode=None,shuffle=False)\n",
    "\n",
    "val_labels = val_gen.classes\n",
    "\n",
    "print('\\nClassifications:\\n',val_gen.class_indices)\n",
    "print('\\nClass labels:\\n',val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "\n",
    "# Part 3: Train the top layer of your CNN\n",
    "\n",
    "Ones you have extracted and written the bottleneck features to files, read them in again and use them to train the top layer of your network, i.e. the small fully-connected model on top of the stored features. When you have done this record and answer with your prediciton accuracy.\n",
    "\n",
    "**Question:** What is the validation accuracy for the last training epoch, and how is it that we can reach such high accuracy with such small amount of data in a short amount of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in bottleneck features\n",
    "# Run the code below to train your CNN with the training data\n",
    "\n",
    "def train_model():\n",
    "    train_data = np.load('features_train.npy')\n",
    "    # the features were saved in order, so recreating the labels is easy\n",
    "    train_labels = np.array([0] * (n_train_samples // 2) + [1] * (n_train_samples // 2))\n",
    "\n",
    "    validation_data = np.load('features_validation.npy')\n",
    "    # same as val_labels above\n",
    "    validation_labels = np.array([0] * (n_validation_samples // 2) + [1] * (n_validation_samples // 2))\n",
    "\n",
    "    # Add top layers trained ontop of extracted VGG features\n",
    "    # Small fully connected model trained on top of the stored features\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    '''\n",
    "    #We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. \n",
    "    #To go with it we will also use the binary_crossentropy loss to train our model.\n",
    "\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    MODEL_WEIGHTS_FILE = 'vgg16-best-weights.h5'\n",
    "    callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', verbose=1, save_best_only=True)]\n",
    "    \n",
    "    history = model.fit(train_data, train_labels, verbose=1, \\\n",
    "                        nb_epoch=20, batch_size=32, \\\n",
    "                        validation_data=(validation_data, validation_labels),\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    # Save weights to disk\n",
    "\n",
    "    # Save model architecture to disk\n",
    "    model_json = model.to_json()\n",
    "    with open(\"mod_appendix.json\", \"w\") as json_file: # save model\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # Save model weights\n",
    "    model.save_weights(\"catvsdogs_VGG16_pretrained_tf_top.h5\") # save weights\n",
    "    print(\"Saved model to disk\")\n",
    "    print('Done!')\n",
    "    \n",
    "    return(history)\n",
    "    \n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.model.load_weights('vgg16-best-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = history.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # only the last layer hsa 2Mn weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({'epoch': range(1,n_epoch+1),\n",
    "                    'training': history.history['acc'],\n",
    "                    'validation': history.history['val_acc']})\n",
    "ax = acc.plot(x='epoch', figsize=(10,6), grid=True)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.7,1.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "\n",
    "# Part 4: Validate accuracy and make predictions on unlabeled data\n",
    "\n",
    "**Question:** First use the model trained in Part 3 to determine the accuracy on the validation data set (is it the same as one the last training epoch?\n",
    "\n",
    "Lastly, use the model trained in Part 3 to classify the test data images. I.e., create a function that loads one image from the test data and then predicts if it is a cat or a dog and with what probability it thinks it is a cat or a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = np.load('features_validation.npy')\n",
    "\n",
    "val_pred_class = model.predict_classes(validation_data,verbose=0) # predict image classes \n",
    "#val_pred_prob = model.predict_proba(validation_data,verbose=0) # predict image probabilities\n",
    "\n",
    "print('Accuracy on validation set: ',np.mean(val_pred_class.ravel()==val_labels)*100,'%')\n",
    "\n",
    "print('\\nVal loss & val_acc')\n",
    "print(model.evaluate(validation_data,val_labels,verbose=0))\n",
    "# First number is validation loss, loss of the objective function\n",
    "# Second number validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "print('Model accuracy on validation set:',model.evaluate(validation_data,val_labels,verbose=0)[1]*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print try images:\n",
    "\n",
    "# Use the model trained in Problem 1 to classify the test data images.\n",
    "# Create a function that loads one image from the test data and then predicts\n",
    "# if it is a cat or a dog and with what probability it thinks it is a cat or a dog\n",
    "#\n",
    "# Use variable test_data to make predictions\n",
    "# Use list test_images to obtain the file name for all images (Note: test_images[0] corresponds to test_data[0])\n",
    "# Use function plot_pic(img) to plot the image file\n",
    "\n",
    "## Load in processed images feature to feed into bottleneck model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "test_data = np.load('features_test.npy')\n",
    "\n",
    "test_images =  [TEST_DIR+'catvdog/'+img for img in sorted(os.listdir(TEST_DIR+'catvdog/'))]\n",
    "\n",
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    im = np.array(Image.open(file_path))\n",
    "    return im\n",
    "\n",
    "def plot_pic(img):\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mod,i=0,r=None):\n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        class_pred = mod.predict_classes(test_data,verbose=0)[idx]\n",
    "        prob_pred = mod.predict_proba(test_data,verbose=0)[idx]\n",
    "        \n",
    "        if class_pred ==0:\n",
    "            prob_pred = 1-prob_pred\n",
    "            class_guess='CAT'\n",
    "        else:\n",
    "            class_guess='DOG'\n",
    "        \n",
    "        print('\\n\\nI think this is a ' + class_guess + ' with ' +str(round(float(prob_pred)*100,5)) + '% probability')\n",
    "        if test_images[idx]=='./data/test/catvdog/.DS_Store' or '.ipynb_checkpoints' in test_images[idx]:\n",
    "            continue\n",
    "        plot_pic(test_images[idx])\n",
    "\n",
    "predict(model,r=range(0,10))       \n",
    "#predict(model,r=range(90,len(test_images))) # seems to be doing really well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "\n",
    "# Part 5 -- right now built for Theano\n",
    "\n",
    "Redo the model and pipeline created in Part 1-4 in order to make predictions on the 50 image classes. Note that you might have to change how you read in the images, so that when you train the model you do a cross validation split (25 / 75) instead of specifying a specific validation set.  And, you will want to use a `softmax` activation layer instead of a `sigmoid` one (to do multiclass classification).\n",
    "\n",
    "The data can be downloaded here: https://www.dropbox.com/s/suy8u0hnthwr2su/50_categories.tar.gz?dl=1\n",
    "\n",
    "Note that you do not have to additional data to make predictions on data not used in the training (however you can easily download 3-5 images like that from Google to try your model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at files, note all cat images and dog images are unique\n",
    "for path, dirs, files in os.walk('./50_categories'):\n",
    "    print('FOLDER',path)\n",
    "    for f in files[:2]:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = next(os.walk('./50_categories'))\n",
    "print(categories)\n",
    "categories = categories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categories to an integer\n",
    "cat_dic = dict()\n",
    "for idx, cat in enumerate(categories):\n",
    "    cat_dic[cat] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs = 0\n",
    "for cat in categories:\n",
    "    nbr_cat_imgs = len(next(os.walk('./50_categories/'+cat))[2])\n",
    "    print('Number of '+cat+' images:', nbr_cat_imgs)\n",
    "    n_imgs+=nbr_cat_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "len(glob(\"./50_categories/*/*.jpg\")) #4244 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with opencv\n",
    "img = cv2.imread('./50_categories/airplanes/airplanes_0001.jpg')\n",
    "img.shape\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all_images = glob(\"./50_categories/*/*.jpg\")\n",
    "path_all_images[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_categories(images):\n",
    "    \"\"\"Get the true categories of a set of paths to images, based on the\n",
    "    directory they are located in.\n",
    "\n",
    "    The paths should have the form:\n",
    "        path/to/image/category/image.jpg\n",
    "\n",
    "    Where the image filename is the last item in the path, and the\n",
    "    directory (category name) is the second to last item in the path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : list\n",
    "        List of paths to images\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    categories : numpy.ndarray\n",
    "        An array of integers in order of the images, corresponding to\n",
    "        each image's category\n",
    "    category_map : list\n",
    "        A list of category names. The category integers in\n",
    "        `categories` are indices into this list.\n",
    "\n",
    "    \"\"\"\n",
    "    get_category = lambda x: os.path.split(os.path.split(x)[0])[1]\n",
    "    categories = list(map(get_category, images))\n",
    "    category_map = sorted(set(categories))\n",
    "    categories = np.array(map(category_map.index, categories))\n",
    "    return categories, category_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cats, img_cat_map = get_image_categories(path_all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cat_map[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "DATA_DIR = './50_categories/'\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "n_samples = n_imgs\n",
    "n_epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Run model once to record the bottleneck features using image data generators:\n",
    "#  Note: This can take a lot of time\n",
    "\n",
    "def save_bottleneck_features():\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # load the weights of the VGG16 networks\n",
    "    # note: when there is a complete match between your model definition\n",
    "    # and your weight savefile, you can simply call model.load_weights(filename)\n",
    "    assert os.path.exists('vgg16_weights.h5'), 'Model weights not found (Download file vgg16_weights.h5 from bcourses).'\n",
    "    f = h5py.File('vgg16_weights.h5')\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "\n",
    "    \n",
    "    # Rescale value we multiply the data before any other processing. \n",
    "    # Our original images consist in RGB coefficients in the 0-255, \n",
    "    # but such values would be too high for our models to process (given typical learning rate), \n",
    "    # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    def generate_features(DIR,n_samples,name_str):\n",
    "        '''\n",
    "        This is a generator that will read pictures found in\n",
    "        subfolers of 'data/*', and indefinitely generate\n",
    "        batches of image rescaled images used to predict\n",
    "        the bottleneck features of the images once\n",
    "        using model.predict_generator(**args**)\n",
    "       '''\n",
    "        print('Generate '+name_str+' image features')\n",
    "\n",
    "        generator = datagen.flow_from_directory(\n",
    "            DIR,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "            shuffle=False)  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "\n",
    "        \n",
    "        features = model.predict_generator(generator, n_samples)\n",
    "        # the predict_generator method returns the output of a model, given\n",
    "        # a generator that yields batches of numpy data\n",
    "        \n",
    "        np.save(open('50_classes_features.npy', 'w'), features) # save bottleneck features to file\n",
    "        \n",
    "    generate_features(DATA_DIR, n_samples, 'data')\n",
    "\n",
    "\n",
    "    \n",
    "save_bottleneck_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain image labels and binary classification\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "class_gen = datagen.flow_from_directory(DATA_DIR,target_size=(img_width, img_height),\n",
    "                                        batch_size=32,class_mode=None,shuffle=False)\n",
    "\n",
    "class_labels = class_gen.classes\n",
    "\n",
    "print('\\nClassifications:\\n',class_gen.class_indices)\n",
    "print('\\nClass labels:\\n',class_labels)\n",
    "\n",
    "print(class_gen.class_indices.keys() == cat_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all of our features are stored in order\n",
    "# and we have not split up our training data into training and validation folders\n",
    "# in order to not only train on the first classes we need to randomize our samples\n",
    "# this can easily be done with scikit-learn's test_train_split module\n",
    "\n",
    "# we train on the X_train and y_train sets\n",
    "# then we evaluate our model on the X_test and y_test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "class_data = np.load('50_classes_features.npy') # load in bottleneck features\n",
    "print(class_data.shape)\n",
    "X_train, X_test, y_train, y_test, path_train, path_test = train_test_split(class_data, class_labels, path_all_images,\\\n",
    "                                                                           test_size=0.2, random_state=150)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_train[0:4])\n",
    "print([img_cat_map[i] for i in y_train[0:4]]) # the img_paths have been mapped correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in bottleneck features\n",
    "# Run the code below to train your CNN with the training data\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "\n",
    "\n",
    "# Add top layers trained ontop of extracted VGG features\n",
    "# Small fully connected model trained on top of the stored features\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=class_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='softmax'))\n",
    "#model.add(Dense(4096, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(4096, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(50, activation='softmax'))\n",
    "\n",
    "    \n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, nb_epoch=40,validation_split=0, verbose=1) # we don't need a validation split\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_pred_class = model.predict_classes(X_test,verbose=1)\n",
    "#val_pred_prob = model.predict_proba(class_data,verbose=0)\n",
    "\n",
    "print(model.evaluate(X_test,y_test,verbose=0))\n",
    "# First number is validation loss, loss of the objective function\n",
    "# Second number validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(val_pred_class)) #ok 50 classes in our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(val_pred_class==y_test)/len(y_test) # ~60% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(X_test,y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    #return cv2.resize(img, (img_height, img_width), interpolation=cv2.INTER_CUBIC)\n",
    "    return img\n",
    "\n",
    "def plot_pic(img):\n",
    "    # Plot openCV pic\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative plotpic function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_pic(img):\n",
    "    image = mpimg.imread(img)\n",
    "    plt.figure(figsize=(9,9))\n",
    "    plt.imshow(image)\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(mod,i=0,r=None):\n",
    "    preds = mod.predict_classes(X_test,verbose=0)\n",
    "    \n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        \n",
    "        img_path = path_test[idx]\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        #x = image.img_to_array(img)\n",
    "        #x = np.expand_dims(x, axis=0)\n",
    "        class_pred = preds[idx]\n",
    "        \n",
    "        class_guess = img_cat_map[class_pred]\n",
    "        \n",
    "        print('\\n\\nProbable category: ' + class_guess)\n",
    "        plot_pic(path_test[idx])\n",
    "\n",
    "predict(model,r=range(0,100))\n",
    "\n",
    "# As we can see below it is pretty accurate, however, for the case when we don't have many \n",
    "# training samples the accuracy is not as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "# Appendix: Training a small convnet from scratch: 80% accuracy in 40 lines of code\n",
    "\n",
    "The right tool for an image classification job is a convnet, so let's try to train one on our data, as an initial baseline. Since we only have few examples, our number one concern should be overfitting. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three, images of people who are sailors, and among them only one lumberjack wears a cap, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
    "\n",
    "Data augmentation is one way to fight overfitting, but it isn't enough since our augmented samples are still highly correlated. Your main focus for fighting overfitting should be the entropic capacity of your model --how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.\n",
    "\n",
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
    "\n",
    "In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).\n",
    "\n",
    "The code snippet below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers. This is very similar to the architectures that Yann LeCun advocated in the 1990s for image classification (with the exception of ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class. This class allows you to:\n",
    "\n",
    "configure random transformations and normalization operations to be done on your image data during training\n",
    "instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs, fit_generator, evaluate_generator and predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image data generator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40, #rotation_range degrees (0-180), range that randomly rotate pictures\n",
    "        width_shift_range=0.2, #width_shift range (fraction of total width) within which to randomly translate pic\n",
    "        height_shift_range=0.2, # -ii-\n",
    "        \n",
    "        #rescale value we multiply the data before any other processing. \n",
    "        #Our original images consist in RGB coefficients in the 0-255, \n",
    "        #but such values would be too high for our models to process (given typical learning rate), \n",
    "        # so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "        rescale=1./255,\n",
    "        \n",
    "        #randomly applying shearing transformations (shear mapping is a linear map that \n",
    "        #displaces each point in fixed direction, by an amount proportional to its \n",
    "        #signed distance from a line that is parallel to that direction)\n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.2, #randomly zooming inside pictures\n",
    "        \n",
    "        #is for randomly flipping half of the images horizontally \n",
    "        #--relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n",
    "\n",
    "        horizontal_flip=True,\n",
    "    \n",
    "        #is the strategy used for filling in newly created pixels, \n",
    "        #which can appear after a rotation or a width/height shift.\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing --we disable rescaling in this case to keep the images displayable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(TRAIN_DIR+'cats/cat0001.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if not os.path.exists('preview'):\n",
    "    os.makedirs('preview')\n",
    "\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "prev_files = next(os.walk('./preview'))[2]\n",
    "print(prev_files[:4])\n",
    "\n",
    "def read_image(file_path):\n",
    "    # For image visualization\n",
    "    im = np.array(Image.open(file_path))\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return im\n",
    "\n",
    "def plot_pic(img):\n",
    "    # Plot openCV pic\n",
    "    pic = read_image(img)    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(pic)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for img in prev_files[:4]:\n",
    "    print('Image '+img)\n",
    "    plot_pic('./preview/'+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, input_shape=(img_height, img_width, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    '''\n",
    "    On top of it we stick two fully-connected layers. \n",
    "    We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. \n",
    "    To go with it we will also use the binary_crossentropy loss to train our model.\n",
    "    '''\n",
    "\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Let's prepare our data. We will use .flow_from_directory()\n",
    "    # to generate batches of image data (and their labels) directly from our jpgs in their respective folders.\n",
    "    \n",
    "    # Below is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    print('Train generator')\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            TRAIN_DIR,  # this is the target directory\n",
    "            target_size=(img_height, img_width),  # all images will be resized to 150x150\n",
    "            batch_size=32,\n",
    "            class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    print('Validation generator')\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "            VAL_DIR,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=32,\n",
    "            class_mode='binary')\n",
    "    \n",
    "\n",
    "    \n",
    "    return model, train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at class indices from our generators\n",
    "\n",
    "\n",
    "_, train_gen,val_gen =first_model()\n",
    "print('')\n",
    "print(val_gen.class_indices)\n",
    "print(val_gen.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit the first model\n",
    "n_epoch = 50 # should be around 50 epochs for 80% accuracy\n",
    "def fit_first_model():\n",
    "\n",
    "    mod1, train_generator, validation_generator = first_model()\n",
    "    mod1.fit_generator(\n",
    "            train_generator,\n",
    "            samples_per_epoch=n_train_samples,\n",
    "            nb_epoch=n_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            nb_val_samples=n_validation_samples)\n",
    "\n",
    "    # save model to disk\n",
    "    mod1.save_weights('w_appendix.h5')  # always save your weights after training or during training\n",
    "    model_json = mod1.to_json()\n",
    "    with open(\"mod_appendix.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "#fit_first_model()\n",
    "print('This has already been run!')\n",
    "\n",
    "### DONE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST MODEL EXPLORATION\n",
    "\n",
    "# load model 1 and weights\n",
    "\n",
    "json_file = open('mod_appendix.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "mod1 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "mod1.load_weights(\"w_appendix.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "mod1.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Extract image features from test set - to make predictions\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "val_generator = datagen.flow_from_directory( VAL_DIR, target_size=(img_height, img_width),\n",
    "                                              batch_size=32,class_mode='binary')\n",
    "\n",
    "preds = mod1.evaluate_generator(val_generator,n_validation_samples)\n",
    "\n",
    "print('\\nModel 1 accuracy on 800 validation images:', round(sum(preds)/2,4)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot picture and print class prediction on cats vs dogs (unsorted)\n",
    "\n",
    "\n",
    "try_images =  [TEST_DIR+'catvdog/'+img for img in os.listdir(TEST_DIR+'catvdog/')]\n",
    "\n",
    "def predict(mod,i=0,r=None):\n",
    "    if r==None:\n",
    "        r=[i]\n",
    "        \n",
    "    for idx in r:\n",
    "        if 'DS_Store' in try_images[idx]:\n",
    "            continue\n",
    "        \n",
    "        img_path = try_images[idx]\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        class_pred = mod.predict_classes(x,verbose=0)\n",
    "        \n",
    "        if class_pred == 0:\n",
    "            class_guess='CAT'\n",
    "        else:\n",
    "            class_guess='DOG'\n",
    "        \n",
    "        print('\\n\\nI think this is a ' + class_guess)\n",
    "        plot_pic(try_images[idx])\n",
    "\n",
    "predict(mod1,r=range(len(try_images)-64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
